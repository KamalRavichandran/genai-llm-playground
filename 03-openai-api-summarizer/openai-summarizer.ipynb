{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamalRavichandran/genai-llm-playground/blob/main/03-openai-api-summarizer/openai-summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yu2VmR4rkNj"
      },
      "source": [
        "# OpenAI parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OYNgnTIu3CB"
      },
      "source": [
        "## Install and upgrade openai package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v5NR_8F6V3wq"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade openai PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh5RzlqFvDl5"
      },
      "source": [
        "## Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dj76hz_ZXtA"
      },
      "outputs": [],
      "source": [
        "import os, PyPDF2\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnBUDQeQvSC-"
      },
      "source": [
        "## Set OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T853DmPwmqR"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"YOUR-OPENAI-API-KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN4_wLQcwpiu"
      },
      "source": [
        "## Set the OpenAI model to be used\n",
        "* \"gpt-4o\" - GPT-4o (“o” for “omni”) is OpenAI's most advanced model. It is multimodal (accepting text or image inputs and outputting text), and it has the same high intelligence as GPT-4 Turbo but is much more efficient—it generates text 2x faster and is 50% cheaper.\n",
        "* \"gpt-4-turbo\" - GPT-4 is a large multimodal model (accepting text or image inputs and outputting text) that can solve difficult problems with greater accuracy than any of the previous models.\n",
        "* \"gpt-3.5-turbo\" - GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNN1HFxwyvBA"
      },
      "outputs": [],
      "source": [
        "MODEL = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the PDF file"
      ],
      "metadata": {
        "id": "8_kdGqmuwDIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Global_Warming.pdf\", 'rb') as pdf_file:\n",
        "  pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "  text = \"\"\n",
        "  # Extract text from each page\n",
        "  for page_num in range(len(pdf_reader.pages)):\n",
        "    page = pdf_reader.pages[page_num]\n",
        "    text += page.extract_text()"
      ],
      "metadata": {
        "id": "4zA6lwjawCsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Krr161WVAc"
      },
      "source": [
        "## Invoke the API call\n",
        "* model - set the model to be used\n",
        "* messages - a list of message objects, where each object has two required fields, role and content\n",
        "* temperature - this param controls the randomness of the generated text. A higher temperature will make the output more random, while a lower temperature will make it more focused and deterministic\n",
        "* max_tokens - the maximum number of tokens that can be generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuFSNb74Z79e"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            # set role as system and content as what exactly is required from it\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful AI assistant. Summarize the provided text\",\n",
        "        },\n",
        "        {\n",
        "            # set role as user and fill in the prompt content which is required to be answered\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Please summarize the following text:\\n{text}\",\n",
        "        },\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=64\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6GoXV57XSnN"
      },
      "source": [
        "## Print the OpenAI API response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo8l35mbaKgY"
      },
      "outputs": [],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}